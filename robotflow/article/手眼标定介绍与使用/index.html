<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  <meta name="author" content="MVIG - Robot AI Team">
  
  
  
  <link rel="prev" href="http://mvig.sjtu.edu.cn/robotflow/article/%E4%BD%BF%E7%94%A8moveit%E6%8E%A7%E5%88%B6ur5/" />
  <link rel="next" href="http://mvig.sjtu.edu.cn/robotflow/article/%E6%9D%9C%E7%BB%9Dtmux%E9%87%8Cconda%E7%8E%AF%E5%A2%83%E9%94%99%E4%B9%B1%E4%B9%8B%E4%B8%80%E5%8A%B3%E6%B0%B8%E9%80%B8%E5%8A%9E%E6%B3%95/" />
  <link rel="canonical" href="http://mvig.sjtu.edu.cn/robotflow/article/%E6%89%8B%E7%9C%BC%E6%A0%87%E5%AE%9A%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           手眼标定介绍与使用 | RobotFlow
       
  </title>
  <meta name="title" content="手眼标定介绍与使用 | RobotFlow">
    
  
  <link rel="stylesheet" href="/robotflow/font/iconfont.css">
  <link rel="stylesheet" href="/robotflow/css/main.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
  <script>
      document.addEventListener("DOMContentLoaded", function() {
          renderMathInElement(document.body, {
              delimiters: [
                  {left: "$$", right: "$$", display: true},
                  {left: "$", right: "$", display: false}
              ]
          });
      });
  </script>
  
  
 

<script type="application/ld+json">
 "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "http:\/\/mvig.sjtu.edu.cn\/robotflow\/"
    },
    "articleSection" : "article",
    "name" : "手眼标定介绍与使用",
    "headline" : "手眼标定介绍与使用",
    "description" : "Robotic Hand-eye Calibration Workspace | Ubuntu 1804 \u0026amp; ROS Melodic Morenia |\nThis repo contains a eye-in-hand calibration tool (Cplusplus \u0026amp; ROS) in JD京东 GRASPING ROBOT CHALLENGE (News),\nand the implements of my paper: Robotic hand-eye calibration with depth camera: A sphere model approach (PDF)\nInside \/src there are 5 ROS packages:\n  rgbd_srv\nused by camera_driver.\n  camera_driver\ndrive Intel® RealSense™ RGBD cameras in ROS.\n(convert raw stream to ros sensor_msgs::Image)",
    "inLanguage" : "en-us",
    "author" : "杨理欣, 钱苏澄",
    "creator" : "杨理欣, 钱苏澄",
    "publisher": "杨理欣, 钱苏澄",
    "accountablePerson" : "杨理欣, 钱苏澄",
    "copyrightHolder" : "杨理欣, 钱苏澄",
    "copyrightYear" : "2020",
    "datePublished": "2020-06-06 00:00:00 \u002b0000 UTC",
    "dateModified" : "2020-06-06 00:00:00 \u002b0000 UTC",
    "url" : "http:\/\/mvig.sjtu.edu.cn\/robotflow\/article\/%E6%89%8B%E7%9C%BC%E6%A0%87%E5%AE%9A%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8\/",
    "wordCount" : "1260",
    "keywords" : [ "ROS","UR5","RealSense", "RobotFlow"]
}
</script>

</head>

  


  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-robot"></i></a>&nbsp;<a href="http://mvig.sjtu.edu.cn/robotflow/">RobotFlow</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/robotflow/knowledge/" title="">知识图谱</a>
                
                <a class="menu-item" href="/robotflow/article/" title="">技术文档</a>
                
                <a class="menu-item" href="/robotflow/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/robotflow/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/robotflow/about/" title="">About</a>
                
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     <div class="container">
        <div class="navbar-header">
            <div>  <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-robot"></i></a>&nbsp;<a href="http://mvig.sjtu.edu.cn/robotflow/">RobotFlow</a></div>
            <div class="menu-toggle">
                <span></span><span></span><span></span>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/robotflow/knowledge/" title="">知识图谱</a>
                
                <a class="menu-item" href="/robotflow/article/" title="">技术文档</a>
                
                <a class="menu-item" href="/robotflow/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/robotflow/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/robotflow/about/" title="">About</a>
                
        </div>
    </div>
</nav>
    	 <main class="main">
          <div class="container">
      		
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">手眼标定介绍与使用</h1>
        <div class="post-meta">
                Written by <a itemprop="name" href="http://mvig.sjtu.edu.cn/robotflow/" rel="author">杨理欣, 钱苏澄</a> with ♥ 
                <span class="post-time">
                on <time datetime=2020-06-06 itemprop="datePublished">June 6, 2020</time>
                </span>
                in
                <i class="iconfont icon-folder"></i>
                <span class="post-category">
                        <a href="http://mvig.sjtu.edu.cn/robotflow/categories/dev/"> dev </a>
                        
                </span>
        </div>
    </header>
    <div class="post-content">
        

        

        
        
     
          
          
          

          
          
          

          <h1 id="robotic-hand-eye-calibration-workspace">Robotic Hand-eye Calibration Workspace</h1>
<p>| <strong><code>Ubuntu 1804 &amp; ROS Melodic Morenia</code></strong> |</p>
<p>This repo contains a eye-in-hand calibration tool (Cplusplus &amp; ROS) in <strong>JD京东 GRASPING ROBOT CHALLENGE (<a href="http://me.sjtu.edu.cn/news/12692.html">News</a>)</strong>,<br>
and the implements of my paper: <strong>Robotic hand-eye calibration with depth camera: A sphere model approach</strong> (<strong><a href="https://ieeexplore.ieee.org/document/8384652/">PDF</a></strong>)</p>
<p>Inside <code>/src</code> there are 5 ROS packages:</p>
<ul>
<li>
<p><strong>rgbd_srv</strong><br>
used by camera_driver.</p>
</li>
<li>
<p><strong>camera_driver</strong><br>
drive Intel® RealSense™ RGBD cameras in ROS.<br>
(convert raw stream to ros <code>sensor_msgs::Image</code>)</p>
</li>
<li>
<p><strong>camera_transform_publisher</strong><br>
publish the transformation matrix (extrinsics) between camera and a marker( chessboard | <a href="https://docs.opencv.org/trunk/d5/dae/tutorial_aruco_detection.html">aruco</a> tag).</p>
</li>
<li>
<p><strong>handeye_calib_marker</strong><br>
handeye calib tools that use RGB camera and a marker, modified from <a href="https://github.com/jhu-lcsr/handeye_calib_camodocal.git">handeye_calib_camodocal</a><br>
<strong>&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-</strong><br>
with the package above you can calibrate the eye-in-hand transformation on RGB images.</p>
</li>
<li>
<p><strong>handeye_calib_sphere</strong><br>
fine-tune the eye-in-hand result based on depth.</p>
</li>
</ul>
<h2 id="prerequisit">Prerequisit</h2>
<ul>
<li>Ubunutu 18.04</li>
<li>ROS Melodic Morenia (desktop-full install)</li>
<li>with OpenCV &amp; Opencv_contrib install in <code>/usr/local</code> (both <code>3.4.0</code>)</li>
<li>librealsense if you use <strong>Intel® RealSense™</strong> RGBD cameras (D400 series and the SR300)</li>
<li>visp_hand2eye_calibration_calibrator<br>
<code>$ sudo apt-get install ros-melodic-visp-hand2eye-calibration</code></li>
<li>glog at <a href="https://github.com/google/glog/releases">here</a> (<code>0.4.0</code>)</li>
<li>Ceres-solver download at <a href="https://github.com/ceres-solver/ceres-solver/releases">here</a> (<code>1.14.0</code>) and instruction at <a href="http://ceres-solver.org/installation.html">here</a></li>
<li>Sophus at <a href="https://github.com/strasdat/Sophus">here</a> (<code>1.0.0</code>)</li>
<li>ros-melodic-cv-bridge *<br>
<code>$ sudo apt-get install ros-melodic-cv-bridge</code></li>
<li>Point Cloud Library (<a href="https://pointclouds.org/">PCL</a>)<br>
<code>$ sudo apt-get install libpcl-dev ros-melodic-pcl-ros</code></li>
</ul>
<h3 id="install-opencv--opencv_contrib">Install opencv &amp; opencv_contrib</h3>
<p>Download <a href="https://github.com/opencv/opencv/releases">opencv</a> and <a href="https://github.com/opencv/opencv_contrib/releases">opencv_contrib</a> (both tested on <code>3.4.0</code>).<br>
Install opencv prerequisit:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>compiler<span style="color:#f92672">]</span> $ sudo apt-get install build-essential
<span style="color:#f92672">[</span>required<span style="color:#f92672">]</span> $ sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev
<span style="color:#f92672">[</span>optional<span style="color:#f92672">]</span> $ sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev
</code></pre></div><p>Build opencv &amp; opencv_contrib from source: (we turned off the cuda options)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ mkdir build <span style="color:#f92672">&amp;&amp;</span> cd build 
$ cmake <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -D CMAKE_BUILD_TYPE<span style="color:#f92672">=</span>Release <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -D OPENCV_EXTRA_MODULES_PATH<span style="color:#f92672">=</span> &lt;path/to/opencv_contrib-3.4.0&gt;/modules/ <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -D BUILD_opencv_cudacodec<span style="color:#f92672">=</span>OFF <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -D WITH_CUDA<span style="color:#f92672">=</span>OFF <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -D WITH_CUBLAS<span style="color:#f92672">=</span>OFF <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -D WITH_CUFFT<span style="color:#f92672">=</span>OFF <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -D ENABLE_PRECOMPILED_HEADERS<span style="color:#f92672">=</span>OFF <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -D CMAKE_INSTALL_PREFIX<span style="color:#f92672">=</span>/usr/local ..

$ make -j5
$ sudo make install
</code></pre></div><h3 id="install-librealsene-if-you-use-intel-realsense-rgbd-camera">Install librealsene (if you use Intel® RealSense™ RGBD Camera)</h3>
<p>We use SR300/D415 camera and use <code>camera_driver</code> package to convert raw RGBD images into ROS topic of <code>sensor_msgs::Image</code>. The <code>librealsense</code> is required only if you use cameras from RealSense™ family, otherwise make your own <code>camera_driver</code>.</p>
<p>Following the librealsense installation guide at <a href="https://github.com/IntelRealSense/librealsense/blob/master/doc/distribution_linux.md">here</a>.<br>
We installed: <code>librealsense2-dkms</code>,  <code>librealsense2-utils</code>, <code>librealsense2-dev</code>,  <code>librealsense2-dbg</code></p>
<h3 id="install-cv_bridge-from-source">Install cv_bridge from source.</h3>
<p>If you are lucky enough, the ros pre-build <code>ros-melodic-cv-bridge</code> works well.<br>
However, since the default OpenCV in <code>ros-melodic-cv-bridge</code> is <code>3.2</code>, while we use  opencv &amp; opencv_contrib in <code>3.4.0</code>, we recommend you to build the cv_bridge from source.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ git clone https://github.com/ros-perception/vision_opencv.git
$ cd vision_opencv
$ git checkout melodic
$ cd cv_bridge

$ mkdir build <span style="color:#f92672">&amp;&amp;</span> cd build
$ cmake ..
$ make -j5
$ sudo make install
</code></pre></div><p>In the meantime, remove the pre-bulid <code>cv_bridge</code> in ros-melodic path <code>/opt/ros/melodic</code>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ cd /opt/ros/melodic

<span style="color:#75715e"># or you can archive them to other place</span>
$ sudo rm -rf lib/libcv_bridge.so
$ sudo rm -rf include/cv_bridge
$ sudo rm -rf share/cv_bridge
</code></pre></div><p>For more information about <code>cv_bridge</code>, see <a href="http://wiki.ros.org/cv_bridge">here</a>.</p>
<h2 id="build">Build</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ git clone https://github.com/lixiny/Handeye-Calibration-ROS.git
$ cd Handeye-Calibration-ROS  
$ catkin_make
</code></pre></div><p>if you only want to calibrate hand-eye transformation on RGB marker,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ git clone https://github.com/lixiny/Handeye-Calibration-ROS.git  
$ cd Handeye-Calibration-ROS  
$ git checkout rgb_only
$ catkin_make
</code></pre></div><h2 id="ready-to-use-example-on-realsense-d415--ur5">Ready to Use (Example on RealSense D415 &amp; UR5)</h2>
<h3 id="0-robot-and-camera-setup">0. Robot and Camera Setup</h3>
<!-- raw HTML omitted -->
<p>cd into the <code>Handeye-Calibration-ROS/</code> folder, and source <code>devel/setup.bash</code> in each new terminal.</p>
<h3 id="1-pluged-in-camera-bringup-realsense">1. Pluged in Camera, Bringup RealSense</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$1 roslaunch camera_driver realsense_driver.launch
</code></pre></div><p>The camera intrinsic (later we call it <code>intr</code>) will appear in current terminal, record it.  <br>
<!-- raw HTML omitted --></p>
<p>The <code>camera_driver</code> publishes 3 topics to rosmaster:</p>
<ul>
<li>/realsense/rgb</li>
<li>/realsense/depth</li>
<li>/realsense/cloud <br>
the point cloud is under tf frame: <code>/camera_link</code></li>
<li>/realsense/camera_info<br>
the camera_info followed ros <a href="http://docs.ros.org/melodic/api/sensor_msgs/html/msg/CameraInfo.html">sensor_msgs/CameraInfo.msg</a>, we only use <code>K</code> and <code>D</code> in it.</li>
</ul>
<p>You can also visualize these topics in <code>rviz</code>.</p>
<h3 id="2-prepare-a-marker">2. Prepare a Marker</h3>
<p>You can either use a chessboard or an aruco plane at <a href="./doc/rawArucoPlane.jpg">doc/rawArucoPlane.jpg</a> we designed.</p>
<h3 id="3-publish-the-transformation-tf-between-camera-and-marker">3. Publish the Transformation (tf) Between Camera and Marker</h3>
<h4 id="--chessboard">😄  Chessboard</h4>
<p>After you placed the chessboard inside camera view, run:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$2 roslaunch camera_transform_publisher chessboard_publisher_realsense.launch

<span style="color:#75715e"># or specify chessboard parameters:</span>
$2 roslaunch camera_transform_publisher chessboard_publisher_realsense.launch <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>        chessboardWidth:<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>  chessboardHeight:<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span>  squareSize:<span style="color:#f92672">=</span>0.02
</code></pre></div><p>[<strong>IMPORTANT</strong>] There are 5 user-specified parameters in <code>chessboard_publisher_realsense.launch</code>:</p>
<ul>
<li>chessboardWidth &ndash; # of inner corners on chessboard width direction</li>
<li>chessboardHeight &ndash; # of inner corners on chessboard height direction</li>
<li>squareSize &ndash; length of side of each square on chessboard, in meter.</li>
<li>cameraTopic &ndash; the rgb topic name published in your camera_driver, in ours: <code>/realsense/rgb</code></li>
<li>cameraInfoTopic &ndash; the camera_info topic name published in your camera_driver, (<code>/realsense/camera_info</code>)</li>
</ul>
<p>Make sure you have already checked them based on your chessboard.</p>
<h4 id="--aruco-plane">😉  Aruco Plane</h4>
<p>After you placed our aruco plane inside camera view, run:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$2 roslaunch camera_transform_publisher aruco_publisher_realsense.launch

<span style="color:#75715e"># or specify aruco plane parameters:</span>
$2 roslaunch camera_transform_publisher aruco_publisher_realsense.launch <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>        tagSideLen:<span style="color:#f92672">=</span>0.035 planeSideLen:<span style="color:#f92672">=</span>0.25 
</code></pre></div><p>[<strong>IMPORTANT</strong>] There are 4 user-specified parameters in <code>aruco_publisher_realsense.launch</code>:</p>
<ul>
<li>tagSideLen &ndash; side length of a single aruco tag, in meter. (see illustration)</li>
<li>planeSideLen &ndash; side length of the whole aruco plane, in meter.(see illustration)</li>
<li>cameraTopic &ndash; the rgb topic name published in your camera_driver, in ours: <code>/realsense/rgb</code></li>
<li>cameraInfoTopic &ndash; the camera_info topic name published in your camera_driver, (<code>/realsense/camera_info</code>)</li>
</ul>
<p>Make sure you have already checked them based on the physical length of your aruco plane.</p>
<p>Now in the pop-up window, you will see an AR cube like this: <br>
<!-- raw HTML omitted --> <!-- raw HTML omitted --> <!-- raw HTML omitted --></p>
<p>This <code>camera_transform_publisher</code> will publish a tf (<code>/camera_link</code>,  <code>/ar_marker</code>) to ros master. If no marker found, a Identity SE3 tf will be used.  You can also visualize the two frame in rviz.</p>
<h3 id="4-bringup-ur">4. Bringup UR</h3>
<p>Followed the instrction <strong>#3. Bringup UR</strong> in <a href="./doc/install_ur.md">doc/install_ur.md</a>.</p>
<h3 id="5-launch-hand-eye-calibration-on-marker">5. Launch Hand-eye Calibration on Marker</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$3 roslaunch handeye_calib_marker handeye_online.launch
</code></pre></div><p>[<strong>IMPORTANT</strong>] There are 4 user-specified parameters in <code>handeye_online.launch</code>:</p>
<ul>
<li>ARTagTF &ndash; the name of marker frame (defined in <code>camera_transform_publisher</code>, <code>/ar_marker</code>)</li>
<li>cameraTF &ndash; the name of camera frame (defined in <code>camera_transform_publisher</code>, <code>/camera_link</code>)</li>
<li>EETF &ndash; the name of End Effector frame (defined by UR, <code>/ee_link</code>)</li>
<li>baseTF &ndash; the name of robot base frame (defined by UR, <code>/base_link</code>)</li>
</ul>
<p>Check these 4 tf names before you launch calibraion!</p>
<h4 id="start-calibration">start calibration</h4>
<p>Repeatedly move UR end effector to different configuration. Meanwhile, make sure at each unique configuration, a valid marker can be detected. In current terminal, press <code>s</code> to record one <code>AX=XB</code> equation. After sufficient # of equations have been recorded (30+), press <code>q</code> to perform calibraition. Then in current terminal, you will see some output like:<br>
<!-- raw HTML omitted --></p>
<h4 id="publish-the-results">publish the results</h4>
<p>modify the file: <code>handeye_calib_marker/launch/show_result.launch</code>,
replace the <code>${Translation: x y z}</code> and <code>${Rotation: qx qy qz qw}</code> based on your terminal output.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-xml" data-lang="xml"><span style="color:#f92672">&lt;launch&gt;</span>
  <span style="color:#f92672">&lt;node</span> <span style="color:#a6e22e">pkg=</span><span style="color:#e6db74">&#34;tf&#34;</span> 
        <span style="color:#a6e22e">type=</span><span style="color:#e6db74">&#34;static_transform_publisher&#34;</span> 
        <span style="color:#a6e22e">name=</span><span style="color:#e6db74">&#34;realsense_link&#34;</span> 
        <span style="color:#a6e22e">args=</span><span style="color:#e6db74">&#34;${Translation: x y z}
</span><span style="color:#e6db74">              ${Rotation: qx qy qz qw}
</span><span style="color:#e6db74">              /ee_link 
</span><span style="color:#e6db74">              /camera_link 
</span><span style="color:#e6db74">              100&#34;</span>
  <span style="color:#f92672">/&gt;</span>
<span style="color:#f92672">&lt;/launch&gt;</span>
</code></pre></div><p>and launch:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$4 roslaunch handeye_calib_marker show_result.launch 
</code></pre></div><p>or, in a new terminal:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># rosrun tf static_transform_publisher \</span>
<span style="color:#75715e">#           x y z qx qy qz qw \</span>
<span style="color:#75715e">#           frame_id  child_frame_id  period_in_ms</span>

$4 rosrun tf static_transform_publisher ＼
        0.0263365 0.0389317 0.0792014 -0.495516 0.507599 -0.496076 0.500715 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>        /ee_link   /camera_link   <span style="color:#ae81ff">100</span>
</code></pre></div><h4 id="visualize-in-rviz">visualize in rviz</h4>
<!-- raw HTML omitted -->
<p>There are several representations/expressions of the HAND-EYE problem:</p>
<ul>
<li>HAND-EYE is the transformation from <code>/ee_link</code> to <code>/camera_link</code></li>
<li>or tf: (<code>/ee_link</code>,  <code>/camera_link</code>)</li>
<li>or father: <code>/ee_link</code>,  child: <code>/camera_link</code></li>
<li>or: P<!-- raw HTML omitted -->ee_link<!-- raw HTML omitted --> = T<!-- raw HTML omitted -->handeye<!-- raw HTML omitted --> * P<!-- raw HTML omitted -->camera_link<!-- raw HTML omitted --></li>
</ul>
<p><strong>NOTE</strong>: now you should have a ready-to-use handeye transformation.<br>
This results is optimized only based on RGB images. You can still fine-tune the result with depth image (if has) following setp # 6.</p>
<h3 id="6-optional-fine-tune-on-sphere">6. (Optional) Fine-tune on Sphere</h3>
<p>// TODO</p>
<h2 id="related-publications">Related Publications:</h2>
<p>Yang, Lixin, et al. &quot; <strong>Robotic hand-eye calibration with depth camera: A sphere model approach.</strong> &quot; 2018 4th International Conference on Control, Automation and Robotics (ICCAR). IEEE, 2018. <strong><a href="https://ieeexplore.ieee.org/document/8384652/">PDF</a></strong></p>
<pre><code>@inproceedings{yang2018robotic,
  title={Robotic hand-eye calibration with depth camera: A sphere model approach},
  author={Yang, Lixin and Cao, Qixin and Lin, Minjie and Zhang, Haoruo and Ma, Zhuoming},
  booktitle={2018 4th International Conference on Control, Automation and Robotics (ICCAR)},
  pages={104--110},
  year={2018},
  organization={IEEE}
}
</code></pre>
<h2 id="license">License</h2>
<p>handeyeCalibWithDepthCamera is freely available for free non-commercial use, and may be redistributed under these conditions. Please, see the <a href="LICENSE">license</a> for further details</p>

    </div>

    <div class="post-copyright">
             
            <p class="copyright-item">
                <span>Author:</span>
                <span>MVIG - Robot AI Team </span>
                </p>
            
           
             
            <p class="copyright-item">
                    <span>Link:</span>
                    <a href=http://mvig.sjtu.edu.cn/robotflow/article/%E6%89%8B%E7%9C%BC%E6%A0%87%E5%AE%9A%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/>http://mvig.sjtu.edu.cn/robotflow/article/%E6%89%8B%E7%9C%BC%E6%A0%87%E5%AE%9A%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/</span>
            </p>
            
             
            <p class="copyright-item lincese">
                本文采用<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可
            </p>
            
    </div>

  
    <div class="post-tags">
        
            <section>
            <i class="iconfont icon-tag"></i>Tag(s): 
            
            <span class="tag"><a href="http://mvig.sjtu.edu.cn/robotflow/tags/ros/">
                    #ROS</a></span>
            
            <span class="tag"><a href="http://mvig.sjtu.edu.cn/robotflow/tags/ur5/">
                    #UR5</a></span>
            
            <span class="tag"><a href="http://mvig.sjtu.edu.cn/robotflow/tags/realsense/">
                    #RealSense</a></span>
            
            </section>
        
        <section>
                <a href="javascript:window.history.back();">back</a></span> · 
                <span><a href="http://mvig.sjtu.edu.cn/robotflow/">home</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="http://mvig.sjtu.edu.cn/robotflow/article/%E4%BD%BF%E7%94%A8moveit%E6%8E%A7%E5%88%B6ur5/" class="prev" rel="prev" title="使用MoveIt控制UR5"><i class="iconfont icon-left"></i>&nbsp;使用MoveIt控制UR5</a>
         
        
        <a href="http://mvig.sjtu.edu.cn/robotflow/article/%E6%9D%9C%E7%BB%9Dtmux%E9%87%8Cconda%E7%8E%AF%E5%A2%83%E9%94%99%E4%B9%B1%E4%B9%8B%E4%B8%80%E5%8A%B3%E6%B0%B8%E9%80%B8%E5%8A%9E%E6%B3%95/" class="next" rel="next" title="杜绝 tmux 中 conda 环境错乱之一劳永逸办法">杜绝 tmux 中 conda 环境错乱之一劳永逸办法&nbsp;<i class="iconfont icon-right"></i></a>
        
    </div>

    <div class="post-comment">
          
                 
          
    </div>
</article>
          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2020 - 2021</span>
        
        <span class="with-love">
    	 <i class="iconfont icon-love"></i> 
         </span>
         
            <span class="author" itemprop="copyrightHolder"><a href="http://mvig.sjtu.edu.cn/robotflow/">MVIG - Robot AI Team</a> | </span> 
         

         
		  <span>Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow">Hugo</a> & <a href="https://github.com/liuzc/leaveit" target="_blank" rel="external nofollow">LeaveIt</a></span> 
    </div>
</footer>












    
    
    <script src="/robotflow/js/vendor_no_gallery.min.js" async=""></script>
    
  



     </div>
  </body>
</html>
